{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0,1,2,3\n",
      "NVIDIA GeForce GTX 1080 Ti\n",
      "NVIDIA GeForce GTX 1080 Ti\n",
      "NVIDIA GeForce GTX 1080 Ti\n",
      "NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "import torchinfo\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from src.AVDataset import AVDataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "       print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs: 6\n",
      "Audio shape: torch.Size([2, 619695])\n",
      "Video frames shape: torch.Size([2, 3216, 1, 84, 84])\n",
      "Audio file: ['../data/audios_denoised_16khz/sub001/sub001_2drt_01_vcv1_r1_video.wav', '../data/audios_denoised_16khz/sub001/sub001_2drt_02_vcv2_r2_video.wav']\n",
      "Video file: ['../data/dataset_2drt_video_only/sub001/2drt/video/sub001_2drt_01_vcv1_r1_video.mp4', '../data/dataset_2drt_video_only/sub001/2drt/video/sub001_2drt_02_vcv2_r2_video.mp4']\n",
      "===========\n",
      "Audio shape: torch.Size([2, 569911])\n",
      "Video frames shape: torch.Size([2, 2960, 1, 84, 84])\n",
      "Audio file: ['../data/audios_denoised_16khz/sub001/sub001_2drt_02_vcv2_r1_video.wav', '../data/audios_denoised_16khz/sub001/sub001_2drt_03_vcv3_r2_video.wav']\n",
      "Video file: ['../data/dataset_2drt_video_only/sub001/2drt/video/sub001_2drt_02_vcv2_r1_video.mp4', '../data/dataset_2drt_video_only/sub001/2drt/video/sub001_2drt_03_vcv3_r2_video.mp4']\n",
      "===========\n",
      "Audio shape: torch.Size([2, 564710])\n",
      "Video frames shape: torch.Size([2, 2932, 1, 84, 84])\n",
      "Audio file: ['../data/audios_denoised_16khz/sub001/sub001_2drt_01_vcv1_r2_video.wav', '../data/audios_denoised_16khz/sub001/sub001_2drt_03_vcv3_r1_video.wav']\n",
      "Video file: ['../data/dataset_2drt_video_only/sub001/2drt/video/sub001_2drt_01_vcv1_r2_video.mp4', '../data/dataset_2drt_video_only/sub001/2drt/video/sub001_2drt_03_vcv3_r1_video.mp4']\n",
      "===========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAudio shape: torch.Size([5, 619695])\\nVideo frames shape: torch.Size([5, 3216, 1, 84, 84]) ... etc\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "# =======\n",
    "audio_root = r\"../data/audios_denoised_16khz\"\n",
    "video_root = r\"../data/dataset_2drt_video_only\"\n",
    "nSubs = [f\"sub{str(i).zfill(3)}\" for i in range(1, 2)]\n",
    "keyword = \"vcv\"\n",
    "dataset = AVDataset(audio_root=audio_root, \n",
    "                    video_root=video_root, \n",
    "                    subs=nSubs, \n",
    "                    filter_keyword=keyword, \n",
    "                    video_max_frames=None, # batch\n",
    "                    audio_sampling_rate=16000,\n",
    "                    frame_skip=1)\n",
    "\n",
    "print(\"Number of pairs:\", len(dataset))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False, pin_memory=True, collate_fn=AVDataset.collate) # Batch / Collation\n",
    "\n",
    "for i, (waveform, frames, audio_path, video_path) in enumerate(dataloader):\n",
    "        print(\"Audio shape:\", waveform.shape) \n",
    "        print(\"Video frames shape:\", frames.shape)\n",
    "        print(\"Audio file:\", audio_path)\n",
    "        print(\"Video file:\", video_path)\n",
    "        print(\"===========\")\n",
    "        if i > 1:\n",
    "            break\n",
    "    \n",
    "\"\"\"\n",
    "Audio shape: torch.Size([5, 619695])\n",
    "Video frames shape: torch.Size([5, 3216, 1, 84, 84]) ... etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple Unet for image\n",
    "# =====================\n",
    "\n",
    "class Simple_UNet(nn.Module):\n",
    "    def __init__(self, base_channels=32):\n",
    "        super(Simple_UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, base_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(base_channels, base_channels*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Bridge\n",
    "        self.bridge = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(base_channels*2, base_channels*4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_channels*4, base_channels*4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*4, base_channels*2, kernel_size=2, stride=2), # can be a bilinear interpolation too\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_channels*2, base_channels*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels*2, base_channels, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Output\n",
    "        self.final = nn.Conv2d(base_channels, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_frames, channels, height, width)#\n",
    "        batch_size, num_frames = x.shape[0], x.shape[1]\n",
    "        \n",
    "        # Reshape to process all frames at once -> (batch_size*num_frames, channels, height, width)\n",
    "        x = x.reshape(-1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        \n",
    "        # Bridge\n",
    "        b = self.bridge(e2)\n",
    "        \n",
    "        # Decoder\n",
    "        d1 = self.dec1(b)\n",
    "        d2 = self.dec2(d1)\n",
    "        \n",
    "        # Output\n",
    "        out = self.final(d2)\n",
    "        \n",
    "        # Reshape back to include frames dimension\n",
    "        out = out.view(batch_size, num_frames, -1, out.shape[2], out.shape[3])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First AV denosiing model\n",
    "# ========================\n",
    "\n",
    "class AVModel(nn.Module):\n",
    "    def __init__(self, base_channels=32):\n",
    "        super().__init__()\n",
    "        self.unet = Simple_UNet(base_channels=base_channels)\n",
    "        \n",
    "        # Projection layers for audio features\n",
    "        self.audio_proj = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # 768 is wav2vec2 feature dim\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 84*84)  # Match video frame size\n",
    "        )\n",
    "        \n",
    "    def forward(self, audio_features, video_frames):\n",
    "        # Process video through UNet\n",
    "        video_output = self.unet(video_frames)  # [B, T, 1, H, W]\n",
    "        \n",
    "        # Project audio features to match video dimensions\n",
    "        B, T, _ = audio_features.shape\n",
    "        audio_proj = self.audio_proj(audio_features)  # [B, T, H*W]\n",
    "        audio_proj = audio_proj.view(B, T, 1, 84, 84)  # Match video dimensions\n",
    "\n",
    "        return video_output, audio_proj\n",
    "    \n",
    "\n",
    "class AVModel2(nn.Module):\n",
    "    def __init__(self, base_channels=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Projection layers for audio features\n",
    "        self.audio_proj = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # 768 is wav2vec2 feature dim\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 84*84)  # Match video frame size\n",
    "        )\n",
    "        \n",
    "    def forward(self, audio_features):\n",
    "        \n",
    "        # Project audio features to match video dimensions\n",
    "        B, T, _ = audio_features.shape\n",
    "        audio_proj = self.audio_proj(audio_features)  # [B, T, H*W]\n",
    "        audio_proj = audio_proj.view(B, T, 1, 84, 84)  # Match video dimensions\n",
    "\n",
    "        return audio_proj\n",
    "    \n",
    "\n",
    "class AVModel3(nn.Module):\n",
    "    def __init__(self, base_channels=32):\n",
    "        super().__init__()\n",
    "        self.unet = Simple_UNet(base_channels=base_channels)\n",
    "        \n",
    "        # Projection layers for audio features\n",
    "        self.audio_proj = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # 768 is wav2vec2 feature dim\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 84*84)  # Match video frame size\n",
    "        )\n",
    "        \n",
    "    def forward(self, audio_features, video_frames):\n",
    "        # Process video through UNet\n",
    "        window_size = 5\n",
    "        T = video_frames.shape[1]\n",
    "        overlap = 1\n",
    "        stride = window_size - overlap\n",
    "        video_outputs = []\n",
    "\n",
    "        # Process frames in sliding windows\n",
    "        for start_idx in range(0, T, stride):\n",
    "            end_idx = min(start_idx + window_size, T)\n",
    "            \n",
    "\n",
    "            window_output = self.unet(video_frames[:, start_idx:end_idx])\n",
    "            \n",
    "            if start_idx == 0:\n",
    "                video_outputs.append(window_output[:, :-overlap] if end_idx < T else window_output)\n",
    "            else:\n",
    "                video_outputs.append(window_output[:, overlap:-overlap] if end_idx < T else window_output[:, overlap:])\n",
    "            \n",
    "            # Free memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Combine all windows\n",
    "        video_output = torch.cat(video_outputs, dim=1)\n",
    "        \n",
    "        # Project audio features to match video dimensions\n",
    "        B, T, _ = audio_features.shape\n",
    "        audio_proj = self.audio_proj(audio_features)  # [B, T, H*W]\n",
    "        audio_proj = audio_proj.view(B, T, 1, 84, 84)  # Match video dimensions\n",
    "\n",
    "        return video_output, audio_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 11.25 MiB is free. Including non-PyTorch memory, this process has 10.89 GiB memory in use. Of the allocated memory 10.59 GiB is allocated by PyTorch, and 142.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m m_ids = [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m]\n\u001b[32m      9\u001b[39m wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mfacebook/wav2vec2-base\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m wav2vec2 = \u001b[43mWav2Vec2Model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/wav2vec2-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43madev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m wav2vec2.eval()\n\u001b[32m     13\u001b[39m model = AVModel3()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/transformers/modeling_utils.py:3162\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3158\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3159\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3160\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3161\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 903 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 11.25 MiB is free. Including non-PyTorch memory, this process has 10.89 GiB memory in use. Of the allocated memory 10.59 GiB is allocated by PyTorch, and 142.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Training 1 DataParallel (discouraged)\n",
    "# ========\n",
    "torch.backends.cuda.max_memory_allocated = 0  # Reset memory stats\n",
    "torch.cuda.empty_cache()  # Clear GPU cache\n",
    "adev = torch.device('cuda:0')\n",
    "mdev = torch.device('cuda:0')\n",
    "m_ids = [1, 2, 0, 3]\n",
    "\n",
    "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(adev)\n",
    "wav2vec2.eval()\n",
    "\n",
    "model = AVModel3()\n",
    "# model= nn.DataParallel(model, device_ids=m_ids) #, output_device=3)\n",
    "model.to(mdev)\n",
    "model.train()\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # more consistent regularization \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Loss functions\n",
    "mse_loss = nn.MSELoss()\n",
    "cosine_loss = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "def train_step(audio_features, video_frames):\n",
    "    # optimizer.zero_grad()\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    \n",
    "    # Forward pass\n",
    "    video_output, audio_proj = model(audio_features, video_frames)\n",
    "    \n",
    "    # Compute losses\n",
    "    # MSE between UNet output and original frames\n",
    "    # print(video_output.shape)\n",
    "    reconstruction_loss = 1 #mse_loss(video_output, video_frames)\n",
    "    \n",
    "    # Cosine similarity between audio projection and video features\n",
    "    # Reshape tensors for cosine similarity\n",
    "    # v_flat = video_output.view(video_output.shape[0], video_output.shape[1], -1)\n",
    "    # a_flat = audio_proj.view(audio_proj.shape[0], audio_proj.shape[1], -1)\n",
    "    alignment_loss = 1 #-cosine_loss(v_flat, a_flat).mean()\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = reconstruction_loss + 0.5 * alignment_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    # total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # mitigate exploding gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # return total_loss.item(), reconstruction_loss.item(), alignment_loss.item()\n",
    "    return total_loss, reconstruction_loss, alignment_loss\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, (waveform, frames, _, _) in enumerate(dataloader):\n",
    "        # Process audio through wav2vec2\n",
    "        with torch.no_grad():\n",
    "            inputs = wav2vec2_processor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(adev) for k, v in inputs.items()}\n",
    "            audio_features = wav2vec2(**inputs).last_hidden_state\n",
    "        \n",
    "        # Move video frames to device\n",
    "        frames = frames.to(mdev)\n",
    "        \n",
    "        # Train step\n",
    "        total_loss, rec_loss, align_loss = train_step(audio_features, frames)\n",
    "        epoch_losses.append(total_loss)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}: Total Loss: {total_loss:.4f}, Rec Loss: {rec_loss:.4f}, Align Loss: {align_loss:.4f}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    # if avg_loss < best_loss:\n",
    "    #     best_loss = avg_loss\n",
    "    #     torch.save(model.state_dict(), 'best_av_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.63 GiB is allocated by PyTorch, and 52.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m m_ids = [\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]\n\u001b[32m     36\u001b[39m wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mfacebook/wav2vec2-base\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m wav2vec2 = \u001b[43mWav2Vec2Model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/wav2vec2-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43madev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m wav2vec2.eval()\n\u001b[32m     40\u001b[39m amod = AVModel2()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/transformers/modeling_utils.py:3162\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3158\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3159\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3160\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3161\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 903 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 10.63 GiB is allocated by PyTorch, and 52.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Training 1 DataParallel (discouraged) ----------------\n",
    "# ========\n",
    "def process_video_in_chunks(unet, video_frames, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Process long video sequences in smaller chunks to avoid memory issues.\n",
    "    Args:\n",
    "        audio_features: Tensor [B, T, D]\n",
    "        video_frames: Tensor [B, T, C, H, W]\n",
    "        chunk_size: Number of frames to process at once\n",
    "    \"\"\"\n",
    "    B, T = video_frames.shape[:2]\n",
    "    outputs = []\n",
    "    \n",
    "    for start_idx in range(0, T, chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, T)\n",
    "        print(video_frames[:, start_idx:end_idx].shape)\n",
    "        # Process chunk\n",
    "        v_out = unet(video_frames[:, start_idx:end_idx])\n",
    "        \n",
    "        outputs.append(v_out)\n",
    "        \n",
    "        # Free memory\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate chunks along temporal dimension\n",
    "    video_output = torch.cat(outputs, dim=1)\n",
    "    \n",
    "    return video_output\n",
    "\n",
    "torch.backends.cuda.max_memory_allocated = 0  # Reset memory stats\n",
    "torch.cuda.empty_cache()  # Clear GPU cache\n",
    "adev = torch.device('cuda:0')\n",
    "mdev = torch.device('cuda:1')\n",
    "m_ids = [1, 0, 2, 3]\n",
    "\n",
    "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(adev)\n",
    "wav2vec2.eval()\n",
    "\n",
    "amod = AVModel2()\n",
    "# model= nn.DataParallel(model, device_ids=m_ids) #, output_device=3)\n",
    "amod.to(adev)\n",
    "\n",
    "model = Simple_UNet(base_channels=32)\n",
    "model = nn.DataParallel(model, device_ids=m_ids) #, output_device=3)\n",
    "model.to(mdev)\n",
    "model.train()\n",
    "    \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # more consistent regularization \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Loss functions\n",
    "mse_loss = nn.MSELoss()\n",
    "cosine_loss = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "def train_step(audio_features, video_frames):\n",
    "    # optimizer.zero_grad()\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    \n",
    "    # Forward pass\n",
    "    \n",
    "    video_output = process_video_in_chunks(model, video_frames)\n",
    "    audio_proj = amod(audio_features)\n",
    "    \n",
    "    # Compute losses\n",
    "    # MSE between UNet output and original frames\n",
    "    reconstruction_loss = mse_loss(video_output, video_frames)\n",
    "    \n",
    "    # Cosine similarity between audio projection and video features\n",
    "    # Reshape tensors for cosine similarity\n",
    "    v_flat = video_output.view(video_output.shape[0], video_output.shape[1], -1)\n",
    "    a_flat = audio_proj.view(audio_proj.shape[0], audio_proj.shape[1], -1)\n",
    "    alignment_loss = -cosine_loss(v_flat, a_flat).mean()\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = reconstruction_loss + 0.5 * alignment_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # mitigate exploding gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss.item(), reconstruction_loss.item(), alignment_loss.item()\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, (waveform, frames, _, _) in enumerate(dataloader):\n",
    "        # Process audio through wav2vec2\n",
    "        with torch.no_grad():\n",
    "            inputs = wav2vec2_processor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(adev) for k, v in inputs.items()}\n",
    "            audio_features = wav2vec2(**inputs).last_hidden_state\n",
    "        \n",
    "        # Move video frames to device\n",
    "        frames = frames.to(mdev)\n",
    "        \n",
    "        # Train step\n",
    "        total_loss, rec_loss, align_loss = train_step(audio_features, frames)\n",
    "        epoch_losses.append(total_loss)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}: Total Loss: {total_loss:.4f}, Rec Loss: {rec_loss:.4f}, Align Loss: {align_loss:.4f}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    # if avg_loss < best_loss:\n",
    "    #     best_loss = avg_loss\n",
    "    #     torch.save(model.state_dict(), 'best_av_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.71 GiB. GPU 0 has a total capacity of 10.90 GiB of which 737.25 MiB is free. Including non-PyTorch memory, this process has 10.18 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 284.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m     audio_features = wav2vec2(**inputs).last_hidden_state\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m video_output, audio_proj = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[32m     61\u001b[39m batch_rec_loss = mse_loss(video_output, sub_frames)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mAVModel.forward\u001b[39m\u001b[34m(self, audio_features, video_frames)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_features, video_frames):\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Process video through UNet\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     video_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, T, 1, H, W]\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Project audio features to match video dimensions\u001b[39;00m\n\u001b[32m     21\u001b[39m     B, T, _ = audio_features.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mSimple_UNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m x = x.view(-\u001b[32m1\u001b[39m, x.shape[\u001b[32m2\u001b[39m], x.shape[\u001b[32m3\u001b[39m], x.shape[\u001b[32m4\u001b[39m])\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m e1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m e2 = \u001b[38;5;28mself\u001b[39m.enc2(e1)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Bridge\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/woody/iwi5/iwi5251h/software/private/conda/envs/aid/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.71 GiB. GPU 0 has a total capacity of 10.90 GiB of which 737.25 MiB is free. Including non-PyTorch memory, this process has 10.18 GiB memory in use. Of the allocated memory 9.63 GiB is allocated by PyTorch, and 284.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Training 2 (sub batch, reducing mem usage)\n",
    "# ========\n",
    "\n",
    "torch.backends.cuda.max_memory_allocated = 0  # Reset memory stats\n",
    "torch.cuda.empty_cache()  # Clear GPU cache\n",
    "adev = torch.device('cuda:0')\n",
    "mdev = torch.device('cuda:0')\n",
    "m_ids = [0,1,2,3]\n",
    "\n",
    "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(adev)\n",
    "wav2vec2.eval()\n",
    "\n",
    "model = AVModel()\n",
    "model= nn.DataParallel(model, device_ids=m_ids) #, output_device=3)\n",
    "model.to(mdev)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # more consistent regularization \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Loss functions\n",
    "mse_loss = nn.MSELoss()\n",
    "cosine_loss = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "# Modified training loop with memory optimizations\n",
    "n_epochs = 50\n",
    "best_loss = float('inf')\n",
    "grad_accumulation_steps = 2  # Gradient accumulation to reduce memory usage\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    # optimizer.zero_grad()\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    \n",
    "    for batch_idx, (waveform, frames, _, _) in enumerate(dataloader):\n",
    "        # Process in smaller chunks if needed\n",
    "        batch_size = waveform.shape[0]\n",
    "        sub_batch_size = 1  # Process 2 samples at a time\n",
    "        \n",
    "        total_loss = 0\n",
    "        rec_loss = 0\n",
    "        align_loss = 0\n",
    "        \n",
    "        for i in range(0, batch_size, sub_batch_size):\n",
    "            end_idx = min(i + sub_batch_size, batch_size)\n",
    "            sub_waveform = waveform[i:end_idx]\n",
    "            sub_frames = frames[i:end_idx].to(mdev)\n",
    "            \n",
    "            # Process audio through wav2vec2\n",
    "            with torch.no_grad():\n",
    "                inputs = wav2vec2_processor(sub_waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "                inputs = {k: v.to(adev) for k, v in inputs.items()}\n",
    "                audio_features = wav2vec2(**inputs).last_hidden_state\n",
    "            \n",
    "            # Forward pass\n",
    "            video_output, audio_proj = model(audio_features, sub_frames)\n",
    "            \n",
    "            # Compute losses\n",
    "            batch_rec_loss = mse_loss(video_output, sub_frames)\n",
    "            \n",
    "            # Reshape tensors for cosine similarity\n",
    "            v_flat = video_output.view(video_output.shape[0], video_output.shape[1], -1)\n",
    "            a_flat = audio_proj.view(audio_proj.shape[0], audio_proj.shape[1], -1)\n",
    "            batch_align_loss = -cosine_loss(v_flat, a_flat).mean()\n",
    "            \n",
    "            # Combined loss\n",
    "            batch_total_loss = batch_rec_loss + 0.5 * batch_align_loss\n",
    "            \n",
    "            # Scale loss and backward pass\n",
    "            scaled_loss = batch_total_loss / grad_accumulation_steps\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            total_loss += batch_total_loss.item()\n",
    "            rec_loss += batch_rec_loss.item()\n",
    "            align_loss += batch_align_loss.item()\n",
    "            \n",
    "            # Clear memory\n",
    "            del video_output, audio_proj, v_flat, a_flat\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update weights after accumulating gradients\n",
    "        if (batch_idx + 1) % grad_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Average losses over sub-batches\n",
    "        total_loss /= (batch_size / sub_batch_size)\n",
    "        rec_loss /= (batch_size / sub_batch_size)\n",
    "        align_loss /= (batch_size / sub_batch_size)\n",
    "        \n",
    "        epoch_losses.append(total_loss)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}: Total Loss: {total_loss:.4f}, Rec Loss: {rec_loss:.4f}, Align Loss: {align_loss:.4f}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    scheduler.step(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_with_sliding_window(model, audio_features, video_frames, window_size=100, overlap=10):\n",
    "    \"\"\"\n",
    "    Process video frames using sliding window approach to reduce memory usage.\n",
    "    \n",
    "    Args:\n",
    "        model: AVModel instance\n",
    "        audio_features: tensor of shape [B, T, 768]\n",
    "        video_frames: tensor of shape [B, T, C, H, W]\n",
    "        window_size: number of frames to process at once\n",
    "        overlap: number of overlapping frames between windows\n",
    "    \"\"\"\n",
    "    B, T, C, H, W = video_frames.shape\n",
    "    stride = window_size - overlap\n",
    "    \n",
    "    # Initialize output tensors\n",
    "    video_outputs = []\n",
    "    audio_projs = []\n",
    "    \n",
    "    # Process each batch independently\n",
    "    for b in range(B):\n",
    "        batch_video_outputs = []\n",
    "        batch_audio_projs = []\n",
    "        \n",
    "        # Process temporal chunks with sliding window\n",
    "        for start_idx in range(0, T, stride):\n",
    "            end_idx = min(start_idx + window_size, T)\n",
    "            \n",
    "            # Extract temporal chunk\n",
    "            video_chunk = video_frames[b:b+1, start_idx:end_idx]\n",
    "            audio_chunk = audio_features[b:b+1, start_idx:end_idx]\n",
    "            \n",
    "            # Process chunk through model\n",
    "            with torch.amp.autocast(\"cuda\"):  # Use mixed precision\n",
    "                v_out, a_proj = model(audio_chunk, video_chunk)\n",
    "            \n",
    "            # Store results\n",
    "            if start_idx == 0:\n",
    "                # For first chunk, keep all frames\n",
    "                batch_video_outputs.append(v_out[:, :-overlap] if end_idx < T else v_out)\n",
    "                batch_audio_projs.append(a_proj[:, :-overlap] if end_idx < T else a_proj)\n",
    "            else:\n",
    "                # For subsequent chunks, remove overlapping frames from the start\n",
    "                batch_video_outputs.append(v_out[:, overlap:-overlap] if end_idx < T else v_out[:, overlap:])\n",
    "                batch_audio_projs.append(a_proj[:, overlap:-overlap] if end_idx < T else a_proj[:, overlap:])\n",
    "        \n",
    "        # Concatenate temporal chunks\n",
    "        video_outputs.append(torch.cat(batch_video_outputs, dim=1))\n",
    "        audio_projs.append(torch.cat(batch_audio_projs, dim=1))\n",
    "        \n",
    "        # Clear memory\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine all batches\n",
    "    video_output = torch.cat(video_outputs, dim=0)\n",
    "    audio_proj = torch.cat(audio_projs, dim=0)\n",
    "    \n",
    "    return video_output, audio_proj\n",
    "\n",
    "# Modified training step\n",
    "def train_step(model, audio_features, video_frames, optimizer, mse_loss, cosine_loss):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Process with sliding window\n",
    "    video_output, audio_proj = process_with_sliding_window(\n",
    "        model, \n",
    "        audio_features, \n",
    "        video_frames,\n",
    "        window_size=150,  # Adjust based on your GPU memory\n",
    "        overlap=10\n",
    "    )\n",
    "    \n",
    "    # Compute losses\n",
    "    reconstruction_loss = mse_loss(video_output, video_frames)\n",
    "    \n",
    "    # Reshape tensors for cosine similarity\n",
    "    v_flat = video_output.view(video_output.shape[0], video_output.shape[1], -1)\n",
    "    a_flat = audio_proj.view(audio_proj.shape[0], audio_proj.shape[1], -1)\n",
    "    alignment_loss = -cosine_loss(v_flat, a_flat).mean()\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = reconstruction_loss + 0.5 * alignment_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss.item(), reconstruction_loss.item(), alignment_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 32           |        cudaMalloc retries: 32        |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1324 MiB |  10870 MiB | 134641 MiB | 133317 MiB |\n",
      "|       from large pool |   1307 MiB |  10865 MiB | 134604 MiB | 133296 MiB |\n",
      "|       from small pool |     16 MiB |     16 MiB |     37 MiB |     20 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1324 MiB |  10870 MiB | 134641 MiB | 133317 MiB |\n",
      "|       from large pool |   1307 MiB |  10865 MiB | 134604 MiB | 133296 MiB |\n",
      "|       from small pool |     16 MiB |     16 MiB |     37 MiB |     20 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1316 MiB |  10865 MiB | 134595 MiB | 133279 MiB |\n",
      "|       from large pool |   1299 MiB |  10860 MiB | 134558 MiB | 133258 MiB |\n",
      "|       from small pool |     16 MiB |     16 MiB |     37 MiB |     20 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  10346 MiB |  10898 MiB | 134586 MiB | 124240 MiB |\n",
      "|       from large pool |  10328 MiB |  10892 MiB | 134548 MiB | 124220 MiB |\n",
      "|       from small pool |     18 MiB |     18 MiB |     38 MiB |     20 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  12013 KiB |  30618 KiB | 459330 KiB | 447317 KiB |\n",
      "|       from large pool |  10672 KiB |  28690 KiB | 405586 KiB | 394914 KiB |\n",
      "|       from small pool |   1341 KiB |   9423 KiB |  53744 KiB |  52403 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     224    |     230    |     553    |     329    |\n",
      "|       from large pool |      21    |      26    |     130    |     109    |\n",
      "|       from small pool |     203    |     204    |     423    |     220    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     224    |     230    |     553    |     329    |\n",
      "|       from large pool |      21    |      26    |     130    |     109    |\n",
      "|       from small pool |     203    |     204    |     423    |     220    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      26    |      27    |     124    |      98    |\n",
      "|       from large pool |      17    |      18    |     105    |      88    |\n",
      "|       from small pool |       9    |       9    |      19    |      10    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       7    |      11    |     103    |      96    |\n",
      "|       from large pool |       2    |       7    |      68    |      66    |\n",
      "|       from small pool |       5    |       9    |      35    |      30    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
